{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection with the D-Wave System\n",
    "This notebook demonstrates the formulation of an $n \\choose k$ optimization problem for solution on a D-Wave quantum computer. The method used in the example problem of this notebook&mdash;feature-selection for machine learning&mdash; is applicable to problems from a wide range of domains; for example financial portfolio optimization. \n",
    "    \n",
    "1. [What is Feature Selection?](#What-is-Feature-Selection?) defines and explains the feature-selection problem.\n",
    "2. [Feature Selection by Mutual Information](#Feature-Selection-by-Mutual-Information) describes a particular method of feature selection that is demonstrated in this notebook.\n",
    "3. [Solving Feature Selection on a Quantum Computer](#Solving-Feature-Selection-on-a-Quantum-Computer) shows how such optimization problems can be formulated for solution on a D-Wave quantum computer. \n",
    "4. [Example Application: Predicting Survival of Titanic Passengers](#Example-Application:-Predicting-Survival-of-Titanic-Passengers) demonstrates the use of *Kerberos*, an out-of-the-box classical-quantum hybrid sampler, to select optimal features for a public-domain dataset. \n",
    "\n",
    "This notebook should help you understand both the techniques and [Ocean software](https://github.com/dwavesystems) tools used for solving optimization problems on D-Wave quantum computers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**New to Jupyter Notebooks?** JNs are divided into text or code cells. Pressing the **Run** button in the menu bar moves to the next cell. Code cells are marked by an \"In: \\[\\]\" to the left; when run, an asterisk displays until code completion: \"In: \\[\\*\\]\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Feature Selection?\n",
    "Statistical and machine-learning models use sets of input variables (\"features\") to predict output variables of interest. Feature selection can be part of the model design process: selecting from a large set of potential features a highly informative subset simplifies the model and reduces dimensionality. \n",
    "\n",
    "For example, to build a model that predicts the ripening of hothouse tomatoes, Farmer MacDonald daily records the date, noontime temperature, daylight hours, degree of cloudiness, rationed water and fertilizer, soil humidity, electric-light hours, etc. These measurements constitute a list of potential features. After a growth cycle or two, her analysis reveals some correlations between these features and crop yields:\n",
    "\n",
    "* fertilizer seems a strong predictor of fruit size\n",
    "* cloudiness and daylight hours seem poor predictors of growth \n",
    "* water rations and soil humidity seem a highly correlated pair of strong predictors of crop rot   \n",
    "\n",
    "Farmer MacDonald suspects that her hothouse's use of electric light reduces dependency on seasons and sunlight. She can simplify her model by discarding date, daylight hours, and cloudiness. She can record just water ration or just soil humidity rather than both.\n",
    "\n",
    "For systems with large numbers of potential input information&mdash;for example, weather forecasting or image recognition&mdash;model complexity and required compute resources can be daunting. Feature selection can help make such models tractable. \n",
    "\n",
    "However, optimal feature selection can itself be a hard problem. This example introduces a powerful method of optimizing feature selection based on a complex probability calculation. This calculation is submitted for solution to a quantum computer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustrative Toy Problem\n",
    "This subsection illustrates the use of feature selection with a simple example: a toy system with a single output generated from three inputs. \n",
    "\n",
    "The model built to predict the system's output is even simpler: it uses just two of three possible features (inputs). You can expect it to perform better when the selected two features are more independent, assuming all three contribute somewhat commensurately to the system output (if an independent feature contributes less than the difference between two dependent ones, this might not be true). In the case of Farmer MacDonalds's tomatoes, a model using rationed water and fertilizer should perform better than one using rationed water and soil humidity. \n",
    "\n",
    "The code cell below uses the NumPy library to define three inputs, the first two of which are very similar: a sine, a noisy sine, and a linear function with added random noise. It defines an output that is a simple linear combination of these three inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sig_len = 100\n",
    "# Three inputs: in1 & in2 are similar \n",
    "in1 = np.sin(np.linspace(-np.pi, np.pi, sig_len)).reshape(sig_len, 1)\n",
    "in2 = np.sin(np.linspace(-np.pi+0.1, np.pi+0.2, sig_len)).reshape(sig_len, 1) + 0.3*np.random.rand(sig_len, 1)\n",
    "in3 = np.linspace(-1, 1, sig_len).reshape(sig_len,1) + 2*np.random.rand(sig_len, 1)\n",
    "\n",
    "out = 2*in1 + 3*in2 + 6*in3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the features and variable of interest (the output). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.plots import plot_toy_signals # To see helper functions, select Jupyter File Explorer View from the Online Learning page\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "# Store problem in a pandas DataFrame for later use\n",
    "toy = pd.DataFrame(np.hstack((in1, in2, in3, out)), columns=[\"in1\", \"in2\", \"in3\", \"out\"])\n",
    "\n",
    "plot_toy_signals(toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `two_var_model` function in the next cell defines a linear model of two variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_var_model(IN, a, b):\n",
    "    ina, inb = IN\n",
    "    return a*ina + b*inb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the SciPy library's `curve_fit` function to try predict the variable of interest from two of three features. The model with less-correlated features performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.plots import plot_two_var_model # To see helper functions, select Jupyter File Explorer View from the Online Learning page\n",
    "import itertools\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "model = []\n",
    "two_vars = []\n",
    "for f0, f1 in itertools.combinations(['in1', 'in2', 'in3'], 2):  \n",
    "    two_vars.append((f0, f1))\n",
    "    popt, pcov = curve_fit(two_var_model, (toy[f0].values, toy[f1].values), toy['out'].values)\n",
    "    model.append(two_var_model((toy[f0].values, toy[f1].values), popt[0], popt[1]).reshape(len(toy), 1))\n",
    "    print(\"Standard deviation for selection of features {} and {} is {:.4f}.\".format(f0, f1, max(np.sqrt(np.diag(pcov)))))\n",
    "model_df = pd.DataFrame(np.hstack(model), columns=two_vars)\n",
    "\n",
    "plot_two_var_model(model_df, toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section introduces a method for optimizing feature selection that can be useful in modeling complex systems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection by Mutual Information\n",
    "There are various methods for [feature selection](https://en.wikipedia.org/wiki/Feature_selection). If you are building a deep-learning network, for example, and have six potential features, you might naively consider training it first on each of the features by itself, then on all 15 combinations of subsets of two features, then 20 combinations of subsets of three features, and so on. However, statistical methods are more efficient.  \n",
    "\n",
    "One statistical criterion that can guide this selection is mutual information (MI). The following subsections explain information and MI with some simple examples.\n",
    "\n",
    "If you already understand MI and Shannon entropy, please skip ahead to section [Solving Feature Selection on a Quantum Computer](#Solving-Feature-Selection-on-a-Quantum-Computer). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantifying Information: Shannon Entropy\n",
    "[Shannon entropy](https://en.wiktionary.org/wiki/Shannon_entropy), $H(X)$,  mathematically quantifies the information in a signal:\n",
    "\n",
    "$H(X) = - \\sum_{x \\in X} p(x) \\log p(x)$\n",
    "\n",
    "where $p(x)$ represents the probability of an event's occurrence. The Shannon Entropy (SE) formula can be understood as weighing by an event's probability a value of $\\log \\frac{1}{p(x)}$ for the event, where the reciprocal is due to the minus sign. This value means that the less likely the occurrence of an event, the more information is attributed to it (intuitively, when a man bites a dog it's news). \n",
    "\n",
    "To calculate SE, the `prob` function defined below calculates probability for a dataset representing some variables (a training set in a machine learning context) by dividing it into bins as a histogram using the NumPy library's `histogramdd` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob(dataset, max_bins=10):\n",
    "    \"\"\"Joint probability distribution P(X) for the given data.\"\"\"\n",
    "\n",
    "    # bin by the number of different values per feature\n",
    "    num_rows, num_columns = dataset.shape\n",
    "    bins = [min(len(np.unique(dataset[:, ci])), max_bins) for ci in range(num_columns)]\n",
    "\n",
    "    prob, _ = np.histogramdd(dataset, bins)\n",
    "    return prob / np.sum(prob)\n",
    "\n",
    "def shannon_entropy(p):\n",
    "    \"\"\"Shannon entropy H(X) is the sum of P(X)log(P(X)) for probabilty distribution P(X).\"\"\"\n",
    "    p = p.flatten()\n",
    "    return -sum(pi*np.log2(pi) for pi in p if pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustration of Shannon Entropy\n",
    "For an intuitive example of measuring SE, this subsection applies the `shannon_entropy` function to three signals with defined distributions:\n",
    "\n",
    "* Uniform: this distribution maximizes values of SE because all outcomes are equally likely, meaning every outcome is equally unpredictable. $H(X) = log(N)$ for uniform distribution, where $N$ is the number of possible outcomes, ${x_1, x_2, ...x_N}$. \n",
    "* Exponential: the steeper the curve, the more outcomes are in the \"tail\" part (have higher probability) with lower information value. \n",
    "* Binomial: the stronger this signal is biased to one outcome, the more predictable its values, the lower its information value. $H(X) = -p \\log(p) - (1-p) \\log(1-p)$ for binomial distribution; for $p = 0.1$, for example, $H(X) = 0.468$.  \n",
    "\n",
    "Define the three signals and plot the SE. The red dots show the maximal values of SE for different numbers of bits (Shannon developed the formula to calculate channel bandwidth, which for digital communications is measured in bits) or, as here, the bins into which the signals' possible values are divided.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.plots import plot_se # To see helper functions, select Jupyter File Explorer View from the Online Learning page\n",
    "\n",
    "max_bins = 10\n",
    "\n",
    "# Signals with defined distributions\n",
    "x_uniform = np.random.uniform(0, 1, (1000, 1))\n",
    "x_exp = np.exp(-np.linspace(0, 10, 1000)/2).reshape(1000, 1)\n",
    "x_vals = np.random.choice([0, 1],(1000, 1), p=[0.1, 0.9])\n",
    "\n",
    "data = list()\n",
    "for bins in range(1, max_bins):\n",
    "    uniform_se = shannon_entropy(prob(x_uniform, bins))\n",
    "    exp_se = shannon_entropy(prob(x_exp, bins))\n",
    "    vals_se = shannon_entropy(prob(x_vals, bins))                               \n",
    "    data.append({'Bins': bins, 'Uniform': uniform_se, 'Maximum': np.log2(bins), 'Exp': exp_se, 'Vals': vals_se})\n",
    "\n",
    "plot_se(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Shannon Entropy\n",
    "\n",
    "Conditional SE (CSE) measures the information in one signal, $X$, when the value of another signal, $Y$, is known: \n",
    "\n",
    "$\\begin{aligned} H(X|Y) \n",
    "& = H(X,Y)-H(Y) \\\\\n",
    "& = - \\sum_{x \\in X} p(x, y) \\log p(x, y) - H(Y) \\end{aligned}$\n",
    "\n",
    "where joint SE, $H(X,Y)$, measures the information in both signals together, with $p(x,y)$ being their joint probability. For example, knowing that it's winter reduces the information value of news that it is raining.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_shannon_entropy(p, *conditional_indices):\n",
    "    \"\"\"Shannon entropy of P(X) conditional on variable j\"\"\"\n",
    "\n",
    "    axis = tuple(i for i in np.arange(len(p.shape)) if i not in conditional_indices)\n",
    "\n",
    "    return shannon_entropy(p) - shannon_entropy(np.sum(p, axis=axis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustration of CSE \n",
    "Apply CSE to the toy problem. Because signals `in1` and `in2` are similar, knowing the value of one provides a good estimate of the other; in contrast, the value of signal `in3` is less good for estimating the first two.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"H(in1) = {:.2f}\".format(shannon_entropy(prob(toy[[\"in1\"]].values))))\n",
    "print(\"H(in1|in3) = {:.2f}\".format(conditional_shannon_entropy(prob(toy[[\"in1\", \"in3\"]].values), 1)))\n",
    "print(\"H(in1|in2) = {:.2f}\".format(conditional_shannon_entropy(prob(toy[[\"in1\", \"in2\"]].values), 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information\n",
    "[Mutual information](https://en.wikipedia.org/wiki/Mutual_information) between variables $X$ and $Y$ is defined as \n",
    "\n",
    "$I(X;Y)  = \\sum_{y \\in Y} \\sum_{x \\in X} p(x, y) \\log \\frac{p(x,y)}{p(x)p(y)}$\n",
    "\n",
    "where $p(x)$ and $p(y)$ are marginal probabilities of $X$ and $Y$, and $p(x,y)$ the joint probability. Equivalently, \n",
    "\n",
    "$I(X;Y)  = H(Y) - H(Y|X)$\n",
    "\n",
    "where $H(Y)$ is the SE of $Y$ and $H(Y|X)$ is the CSE of $Y$ conditional on $X$.\n",
    "\n",
    "Mutual information (MI) quantifies how much one knows about one random variable from observations of another. Intuitively, a model based on just one of a pair of features will better reproduce their combined contribution when MI between them is high. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_information(prob, j):\n",
    "    \"\"\"Mutual information between all variables and variable j\"\"\"\n",
    "    return shannon_entropy(np.sum(prob, axis=j)) - conditional_shannon_entropy(prob, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MI on the Toy Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate and plot MI between the output of the toy problem and its three input signals. This measures the suitability of each on its own as a feature in a model of the system, or how much each shapes the output.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from helpers.plots import plot_mi # To see helper functions, select Jupyter File Explorer View from the Online Learning page\n",
    "\n",
    "mi = {}\n",
    "for column in toy.columns:\n",
    "    if column == 'out':\n",
    "        continue\n",
    "    mi[column] = mutual_information(prob(toy[['out', column]].values), 0)\n",
    "\n",
    "plot_mi(mi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot of input and output signals in the first section might give an impression that the toy model's output is closer to the two sine signals than to `in3`, but the linear regression below confirms the MI result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "from helpers.plots import plot_lingress # To see helper functions, select Jupyter File Explorer View from the Online Learning page\n",
    "\n",
    "model = []\n",
    "var_rval = []\n",
    "for column in toy.columns:\n",
    "    if column == 'out':\n",
    "        continue\n",
    "    slope, intercept, rvalue, *_ = linregress(toy[column].values, toy['out'].values)  \n",
    "    model.append((slope*toy[column].values + intercept).reshape(len(toy), 1))\n",
    "    var_rval.append((column, rvalue))\n",
    "\n",
    "plot_lingress(pd.DataFrame(np.hstack(model), columns=var_rval), toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result should in fact be expected given an output defined as $out = 2 in1 + 3 in2 + 6 in3$, with the sixfold multiplier on $in3$ greater than the sum of multipliers on the other signals, all three of which have an amplitude of 1.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Mutual Information\n",
    "\n",
    "Conditional mutual information (CMI) between a variable of interest, $X$, and a feature, $Y$, given the selection of another feature, $Z$, is given by\n",
    "\n",
    "$I(X;Y|Z) = H(X|Z)-H(X|Y,Z)$\n",
    "\n",
    "where $H(X|Z)$ is the CSE of $X$ conditional on $Z$ and $H(X|Y, Z)$ is the CSE of $X$ conditional on both $Y$ and $Z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_mutual_information(p, j, *conditional_indices):\n",
    "    \"\"\"Mutual information between variables X and variable Y conditional on variable Z.\"\"\"\n",
    "\n",
    "    return conditional_shannon_entropy(np.sum(p, axis=j), *conditional_indices) - conditional_shannon_entropy(p, j, *conditional_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply `conditional_mutual_information` to the toy problem to find CMI between `out` and `in1` conditional on either `in2`, which is similar to `in1`, or `in3`, which is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"I(out;in1|in2) = {:.2f}\".format(conditional_mutual_information(prob(toy[['out', 'in1', 'in2']].values), 1, 2)))\n",
    "print(\"I(out;in1|in3) = {:.2f}\".format(conditional_mutual_information(prob(toy[['out', 'in1', 'in3']].values), 1, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, to select a model's $k$ most relevant of $n$ features, you could maximize $I({X_k}; Y)$, the MI between a set of $k$ features, $X_k$, and variable of interest, $Y$. This is a hard calculation because $n \\choose k$ grows rapidly in real-world problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Feature Selection on a Quantum Computer\n",
    "There are different methods of approximating the hard calculation of optimally selecting $n \\choose k$ features to maximize MI. One approach assumes conditional independence of the features and limits CMI calculations to permutations of three features. The optimal set of features is then approximated by:\n",
    "\n",
    "$\\arg \\max_s \\sum_{i=1}^n \\left \\{ I(X_i;Y) + \\sum_{j \\in s|i} I(X_j;Y |X_i) \\right \\}$\n",
    "\n",
    "\n",
    "The lefthand component, $I(X_i;Y)$, represents MI between the variable of interest and a particular feature; maximizing means selecting features that are highly predictive of the output. The righthand component, $I(X_j;Y |X_i)$, represents conditional MI between the variable of interest and a feature given the prior selection of another feature; maximizing means selecting features that complement information about the output variable rather than provide redundant information.\n",
    "\n",
    "This approximation is still a hard calculation. The following subsection demonstrates a method for formulating it for solution on the D-Wave quantum computer. The method is based on the 2014 paper, [Effective Global Approaches for Mutual Information Based Feature Selection](#https://dl.acm.org/citation.cfm?id=2623611), by Nguyen, Chan, Romano, and Bailey published in the Proceedings of the 20th ACM SIGKDD international conference on knowledge discovery and data mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIQUBO: a QUBO Representation of Feature Selection\n",
    "D-Wave systems solve binary quadratic models (BQM): the Ising model traditionally used in statistical mechanics and its computer-science equivalent, the quadratic unconstrained binary optimization (QUBO) problem. Given $N$ variables $x_1,...,x_N$, where each variable $x_i$ can have binary values $0$ or $1$, the system finds assignments of values that minimize,\n",
    "    \n",
    "$\\sum_i^N q_ix_i + \\sum_{i<j}^N q_{i,j}x_i  x_j$,\n",
    "    \n",
    "where $q_i$ and $q_{i,j}$ are configurable (linear and quadratic) coefficients. To formulate a problem for the D-Wave system is to program $q_i$ and $q_{i,j}$ so that assignments of $x_1,...,x_N$ also represent solutions to the problem.\n",
    "\n",
    "For feature selection, the Mutual Information QUBO (MIQUBO) method formulates a QUBO based on the approximation above for $I({X_s}; Y)$, which can be submitted to the D-Wave quantum computer for solution.\n",
    "\n",
    "The approximation above for optimal feature selection, with its reduction of scope to permutations of three variables, is a natural fit for QUBO formulation. \n",
    "\n",
    "<table style=\"width:75%\">\n",
    "  <tr>\n",
    "    <th width=\"15%\"></th>\n",
    "    <th width=\"25%\">Formula</th>\n",
    "    <th width=\"10%\">Optimization</th> \n",
    "    <th width=\"10%\">Linear terms</th>\n",
    "    <th width=\"15%\">Quadratic terms</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Feature Selection</b></td> \n",
    "    <td>$\\sum_{i=1}^n \\left \\{ I(X_i;Y) + \\sum_{j \\in s|_i} I(X_j;Y |X_i) \\right \\}$</td>\n",
    "    <td>Maximize</td> \n",
    "    <td>$I(X_i;Y)$</td>\n",
    "    <td>$I(X_j;Y |X_i)$</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>QUBO</b></td>\n",
    "    <td>$\\sum_i^N q_ix_i + \\sum_{i<j}^N q_{i,j}x_i  x_j$</td>\n",
    "    <td>Minimize</td> \n",
    "    <td>$q_ix_i$</td>\n",
    "    <td>$q_{i,j}x_ix_j$</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "You can represent the choice of $n \\choose k$ features in the value of solution $x_1,...,x_N$ by encoding $x_i$ as $1$ if feature $X_i$ should be selected and as $0$ if not. With solutions encoded this way, you can represent the QUBO in matrix format, $\\mathbf{x}^T \\mathbf{Q x}$, where $\\mathbf Q$ is an $n$ X $n$ matrix and $\\mathbf{x}$ is an $n$ x $1$ matrix (a vector) that should have $k$ ones representing the selected features. \n",
    "\n",
    "To map the approximate feature-selection formula above to a QUBO, set the elements of $\\mathbf Q$ such that\n",
    "\n",
    " * diagonal elements (linear coefficients) maximize MI: $Q_{ii} \\leftarrow -I(X_i;Y)$ \n",
    " * non-diagonal elements (quadratic elements) maximize CMI: $Q_{ij} \\leftarrow -I(X_j;Y |X_i)$.\n",
    "\n",
    "where the QUBO terms are negative because the quantum computer seeks to minimize the programmed problem while the feature-selection formula maximizes. The following subsection codes this and completes the formulation by mapping the restriction of $n \\choose k$ to the QUBO.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIQUBO on the Toy Problem\n",
    "This subsection applies the MIQUBO formulation to the toy problem by configuring the QUBO in three parts: (1) linear biases that maximize MI between the variable of interest and each feature (2) quadratic biases that maximize CMI between the variable of interest and each feature, given the prior choice of another feature (3) selection of just $k$ features.\n",
    "\n",
    "Create a BQM and set the linear coefficients as the MI between `out` and each potential feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dimod\n",
    "bqm = dimod.BinaryQuadraticModel.empty(dimod.BINARY)\n",
    "\n",
    "for column in toy.columns:\n",
    "\n",
    "    if column == 'out':\n",
    "        continue\n",
    "\n",
    "    mi = mutual_information(prob(toy[['out', column]].values), 1)\n",
    "    bqm.add_variable(column, -mi)\n",
    "\n",
    "for item in bqm.linear.items():\n",
    "    print(\"{}: {:.3f}\".format(item[0], item[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the quadratic coefficients as the MI between `out` and each potential feature conditional on the other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f0, f1 in itertools.combinations(['in1', 'in2', 'in3'], 2):\n",
    "    cmi = conditional_mutual_information(prob(toy[['out', f0, f1]].values), 1, 2)\n",
    "    bqm.add_interaction(f0, f1, -cmi)\n",
    "\n",
    "bqm.normalize()\n",
    "\n",
    "for item in bqm.quadratic.items():\n",
    "    print(\"{}: {:.3f}\".format(item[0], item[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find an exact solution to the BQM, which currently represents a maximization of MI and CMI, using *ExactSampler()* from Ocean software's [dimod](https://docs.ocean.dwavesys.com/projects/dimod/en/latest/) tool and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.plots import plot_solutions # To see helper functions, select Jupyter File Explorer View from the Online Learning page\n",
    "\n",
    "sampler = dimod.reference.samplers.ExactSolver()\n",
    "\n",
    "result = sampler.sample(bqm)\n",
    "\n",
    "plot_solutions(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, the best solution (lowest energy solution for the minimized QUBO) employs all three input signals because the current QUBO mapping does not restrict the number of selected features. In those solutions where only two are selected, models that select `in3` perform better than the one that selects just `in1` and `in2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penalizing Non-k Selections\n",
    "How do you program the requirement that $k$ features be selected on the quantum computer? By penalizing solutions that select greater or fewer than $k$ features. If you add \n",
    "\n",
    "$P = \\alpha \\sum_{i=1}^n ( x_i - k)^2$ \n",
    "\n",
    "to the QUBO, where penalty $P$ is positive whenever the number of $1$ values in solution $x_1,...,x_N$ is not $k$, such solutions are no longer minima for the problem.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "bqm.update(dimod.generators.combinations(bqm.variables, k, strength=4))\n",
    "\n",
    "result = sampler.sample(bqm)\n",
    "\n",
    "plot_solutions(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Application: Predicting Survival of Titanic Passengers\n",
    "This example illustrates the MIQUBO method by finding an optimal feature set for predicting survival of Titanic passengers. It uses records provided in file `formatted_titanic.csv`, which is a feature-engineered version of a public database of passenger information recorded by the ship's crew. In addition to a column showing survival for each passenger, its columns record gender, title, class, port of embarkation, etc. \n",
    "\n",
    "The next cell reads the file into a pandas DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv(\"data/formatted_titanic.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a ranking of all the features by MI with the variable of interest (survival)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi = {}\n",
    "features = list(set(titanic.columns).difference(('survived',)))\n",
    "\n",
    "for feature in features:\n",
    "    mi[feature] = mutual_information(prob(titanic[['survived', feature]].values), 0)\n",
    "\n",
    "plot_mi(mi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact Versus Good Solutions: a Note on this Dataset\n",
    "This example demonstrates a technique for solving a type of optimization problem on a quantum computer. The Titanic dataset was chosen to provide a familiar, intuitive example available in the public domain. In itself, however, it is not a good fit for solving by sampling. \n",
    "\n",
    "The next cell plots the values of MI and CMI for all features and three-variable permutations. Notice the clustering of values in tight ranges of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(len(features)), [mutual_information(prob(titanic[['survived', feature]].values), 0) for feature in features], 'bo')\n",
    "plt.plot(range(len([x for x in itertools.combinations(features, 2)])), [conditional_mutual_information(prob(titanic[['survived', f0, f1]].values), 0, 2) for f0, f1 in itertools.combinations(features, 2)], 'go')\n",
    "plt.title(\"Titanic MI & CMI Values\")\n",
    "plt.ylabel(\"MI/CMI\")\n",
    "plt.xlabel(\"Variable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below, obtained by exploiting the problem's small size and brute-force solving for all possible values, shows the solution space for a couple of choices of $k$. The left side shows the resulting energy for all possible assignments of values to $x_1...x_N$ (yellow) and those that satisfy the requirement of $k \\choose n$ (blue); the right side focusses on only those that satisfy $k \\choose n$ and highlights the optimal solution (red).\n",
    "\n",
    "<img src=\"images/k4_7_solution_space.png\" width=800x>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the high number of valid solutions that form a small cluster (the energy difference between the five best solutions for the depicted run was in the fourth decimal place). The quantum computer's strength is in quickly finding diverse good solutions to hard problems, it is not best employed as a double-precision numerical calculator. Run naively on this dataset it finds numerous good solutions but is unlikely to find the exact optimal solution.\n",
    "\n",
    "There are many techniques for reformulating problems for the D-Wave system that can improve performance on various metrics, some of which can help narrow down good solutions to closer approach an optimal solution. These are out of scope for this example. For more information, see Leap's other Jupyter Notebooks, the [D-Wave Problem-Solving Handbook](https://docs.dwavesys.com/docs/latest/doc_handbook.html), and examples in the [Ocean software documentation](https://docs.ocean.dwavesys.com/en/latest/).\n",
    "\n",
    "The remainder of this section solves the problem for just the highest-scoring features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the MI-Based BQM\n",
    "Select the best 8 features based on the MI ranking found above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = 8\n",
    "\n",
    "titanic = titanic[[column[0] for column in sorted(mi.items(), key=lambda pair: pair[1], reverse=True)][0:keep] + [\"survived\"]]\n",
    "features = list(set(titanic.columns).difference(('survived',)))\n",
    "print(\"Submitting for {} features: {}\".format(keep, features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate a BQM based on the problem's MI and CMI as done previously for the toy problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.draw import plot_bqm # To see helper functions, select Jupyter File Explorer View from the Online Learning page\n",
    "\n",
    "bqm = dimod.BinaryQuadraticModel.empty(dimod.BINARY)\n",
    "\n",
    "# add the features\n",
    "for feature in features:\n",
    "    mi = mutual_information(prob(titanic[['survived', feature]].values), 0)\n",
    "    bqm.add_variable(feature, -mi)\n",
    "\n",
    "for f0, f1 in itertools.combinations(features, 2):\n",
    "    mi = conditional_mutual_information(prob(titanic[['survived', f0, f1]].values), 0, 2)\n",
    "    bqm.add_interaction(f0, f1, -mi)\n",
    "\n",
    "bqm.normalize()  # to -1, 1\n",
    "\n",
    "plot_bqm(bqm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up a QPU as a Solver\n",
    "A solver is a resource that runs problems; for example, a D-Wave system. Typically with Ocean tools you configure a default solver and that configuration is used implicitly (for example, your API token for authentication). Occasionally you may wish to override a default and specify particular solver settings.\n",
    "\n",
    "Your default token is used in the next cell, where *DWaveSampler()* from Ocean software's [dwave-system](https://docs.ocean.dwavesys.com/projects/system/en/latest/) tool handles the connection to a D-Wave system (the `solver={'qpu': True}` argument).\n",
    "\n",
    "Mapping between the problem's variables to the D-Wave QPU's numerically indexed qubits, *minor-embedding*, can be handled in a variety of ways and this affects solution quality and performance. Ocean software provides tools suited for different types of problems; for example, [dwave-system](https://docs.ocean.dwavesys.com/projects/system/en/latest/) *EmbeddingComposite()* has a heuristic for automatic embedding. This example uses *FixedEmbeddingComposite()* minor-embedding on an embedding found for a clique (complete graph) of the relevant number of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dwave.system import DWaveSampler, FixedEmbeddingComposite\n",
    "from dwave.embedding.chimera import find_clique_embedding\n",
    "\n",
    "qpu = DWaveSampler(solver={'qpu': True})\n",
    "\n",
    "embedding = find_clique_embedding(bqm.variables,\n",
    "                                  16, 16, 4,  # size of the chimera lattice\n",
    "                                  target_edges=qpu.edgelist)\n",
    "\n",
    "qpu_sampler = FixedEmbeddingComposite(qpu, embedding)\n",
    "\n",
    "print(\"Maximum chain length for minor embedding is {}.\".format(max(len(x) for x in embedding.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem is small enough to be solved in its entirety on the QPU. For datasets with higher numbers of features, D-Wave Ocean's [dwave-hybrid](https://docs.ocean.dwavesys.com/projects/hybrid/en/latest/) tool can be used to break the BQM into smaller peices for serial submission to the QPU and/or solution on classical resources in parallel. Here, an out-of-the-box hybrid sampler, *Kerberos* is used.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the Problem for All k Values\n",
    "For every desired number of features, $k$, set a $n \\choose k$ penalty, submit an updated BQM for solution, and at the end plot the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.draw import plot_feature_selection # To see helper functions, select Jupyter File Explorer View from the Online Learning page\n",
    "\n",
    "selected_features = np.zeros((len(features), len(features)))\n",
    "for k in range(1, len(features) + 1):\n",
    "    print(\"Submitting for k={}\".format(k))\n",
    "    kbqm = dimod.generators.combinations(features, k, strength=6)\n",
    "    kbqm.update(bqm)\n",
    "    kbqm.normalize()\n",
    "    \n",
    "    best = qpu_sampler.sample(kbqm, num_reads=10000).first.sample\n",
    "    \n",
    "    for fi, f in enumerate(features):\n",
    "        selected_features[k-1, fi] = best[f]\n",
    "\n",
    "plot_feature_selection(features, selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.draw import plot_feature_selection # To see helper functions, select Jupyter File Explorer View from the Online Learning page\n",
    "\n",
    "from hybrid.reference.kerberos import KerberosSampler\n",
    "import functools\n",
    "sampler_with_chainstrength = qpu_sampler\n",
    "sampler_with_chainstrength.sample = functools.partial(qpu_sampler.sample, chain_strength=4)\n",
    "\n",
    "kerby = KerberosSampler()\n",
    "\n",
    "selected_features = np.zeros((len(features), len(features)))\n",
    "selected_features_check = np.zeros((len(features), len(features)))\n",
    "for k in range(1, len(features) + 1):\n",
    "    print(\"Submitting for k={}\".format(k))\n",
    "    kbqm = dimod.generators.combinations(features, k, strength=6)\n",
    "    kbqm.update(bqm)\n",
    "    kbqm.normalize()\n",
    "    \n",
    "    result = kerby.sample(kbqm, qpu_sampler=sampler_with_chainstrength, qpu_reads=10000, num_reads=1, max_iter=1, convergence=3)\n",
    "    #result = qpu_sampler.sample(kbqm, num_reads=10000)\n",
    "    best = result.first.sample\n",
    "    \n",
    "    result_check = dimod.ExactSolver().sample(kbqm)\n",
    "    best_check = result_check.first.sample\n",
    "    \n",
    "    print(\"k: {}, found energy: {:.5f} {} time versus ground {:.5f}\".format(k, result.first.energy, result.first.num_occurrences, result_check.first.energy))\n",
    "\n",
    "    \n",
    "    for fi, f in enumerate(features):\n",
    "        selected_features[k-1, fi] = best[f]\n",
    "        selected_features_check[k-1, fi] = best_check[f] \n",
    "\n",
    "plot_feature_selection(features, selected_features)\n",
    "plot_feature_selection(features, selected_features_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hybrid\n",
    "hybrid.print_counters(kerby.runnable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
